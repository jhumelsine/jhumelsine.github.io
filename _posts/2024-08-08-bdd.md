---
title: DRAFT – Be On Your Best Behavior
description: Test-Driven Development - How to create test; Behavior-Driven Development - What tests to create
unlisted: true
---

# Introduction
Previous blogs have addressed the [Attributes of Effective Unit Tests](https://jhumelsine.github.io/2024/06/14/unit-test-attributes.html), the [Basic Elements of Automated Unit Tests](https://jhumelsine.github.io/2024/06/23/unt-test-elements.html), including [Test Doubles](https://jhumelsine.github.io/2024/07/02/test-doubles.html), the process for [Writing Unit Tests](https://jhumelsine.github.io/2024/07/15/tdd.html) as well as a couple stories with [Suril](https://jhumelsine.github.io/2024/07/08/suril-semaphore.html) and [Yuri](https://jhumelsine.github.io/2024/07/22/yuri-evaluator.html).

I haven’t spent much time writing about how to determine what to test. This blog will focus upon that task with [Behavior-Driven Development](https://en.wikipedia.org/wiki/Behavior-driven_development) (BDD).

# How Do We Know Our Code Is Correct
As mentioned in the [Formal Proofs]( https://jhumelsine.github.io/2024/06/07/unit-test-convert.html#formal-proofs) section of the first blog in this testing series, I was taught formal algorithm proving techniques in college. But regardless of how confident we felt in our reasoning, a proof never seemed sufficient. We still needed to see the code run.

Traditional computer science problems, such as data structures and algorithms, are: well defined, unambiguous, limited scoped, and unchanging problems. Their solutions lend themselves to formal proofs.

_Testing can show the presence of bugs, but not their absence!_ — __Edsger W. Dijkstra__

[Bob Martin](https://en.wikipedia.org/wiki/Robert_C._Martin) provided context to Dijkstra’s quote in his ___Clean Architecture___ book in _Chapter 4 – Structured Programming_. Dijkstra believed in formal proofs. His quote is a bit of a jab at the testing community.

Dijkstra’s statement is not incorrect. Testing never shows the absence of bugs, but formal proofs don’t tend to work for software engineering problems, which tend to be fuzzy, ambiguous, large scoped and changing.

What's a software developer to do? Change his or her mind set. While computer science problems mirror mathematical domains where rigorous proofs prove correctness, software engineering problems mirror scientific domains from which we gain confidence via the scientific method.

# Scientific Method
The [Scientific Method]( https://en.wikipedia.org/wiki/Scientific_method) doesn’t prove a scientific observation or principle. It’s a process where scientific models are subjected to experiments attempting to invalidate them. Models that fail experiments are discarded or modified. Models that pass experiments gain more acceptance in being useful models.

# Behavior-Driven Development
Software engineering can adopt a similar strategy. [Behavior-Driven Development](https://en.wikipedia.org/wiki/Behavior-driven_development) (BDD) mirrors the scientific method. BDD tests are experiments that focus upon behavior.

__BDD tests specify behavior for the [Software Under Test](https://en.wikipedia.org/wiki/System_under_test) (SUT), whose implementation is challenged and confirmed each time the tests are executed.__ Consistency between the behavior specification defined in the test and the behavior observation implemented in the code is confirmed each time the tests are executed. Bob Martin uses [Double-Entry Bookkeeping](https://blog.cleancoder.com/uncle-bob/2017/12/18/Excuses.html) as an analogy.

BDD tests tend to fall into two broad categories:
* SUT behavior specificaiton tests, which tends to cover the general cases
* SUT behavior challenge tests, which tends to conver the edge cases

As more tests pass, we gain more confidence that the implementation aligns with the specified behavior. We can never achieve 100% confidence, but our confidence level may be good enough. 

_Perfect is the enemy of good._ — __Voltaire__

## Contract
SUT behavior is declared as a contract usually via public methods. A contract defines expectations and obligations. These expectations and obligations apply to both the provider of the SUT and the client that interacts with the SUT.

The contract should be independent of the implementation. A contract specifies behavior. It does not specify how the behavior is to be implemented.

I will write a blog dedicated to contracts in the future (TBD).

## Behavior-Driven Development Form
BDD test names summarize the behavior specification. The test name is often declared as a method name. They tend to be longer than traditional method names. Here are a few examples:
* `sum_Returns4_WhenOperandsAre2And2()`
* `orderBook_CreatesBookOrderEvent_WhenBookIsInStock()`
* `orderBook_CreatesBookReorderEvent_WhenBookIsNotInStock()`

Non-BDD test names might be:
* `testSum()`
* `orderBook1()`
* `orderBook2()`

I jumped the gun a bit in a previous blog where I described the structure of BDD [Test Elements](https://jhumelsine.github.io/2024/06/23/unt-test-elements.html#test-elements), even if I didn’t mention BDD by name. BDD tests are written in the [Given-When-Then](https://en.wikipedia.org/wiki/Given-When-Then) structure:
* The __Given__ portion is where the test [Set Up](https://jhumelsine.github.io/2024/06/23/unt-test-elements.html#set-up--ie-arrangegiven) occurs. This is usually declaring and defining [Test Doubles](https://jhumelsine.github.io/2024/07/02/test-doubles.html).
* The __When__ portion is where the SUT is [Executed]( https://jhumelsine.github.io/2024/06/23/unt-test-elements.html#execution--ie-actwhen).
* The __Then__ portion is where the observable behaviors are [Confirmed](https://jhumelsine.github.io/2024/06/23/unt-test-elements.html#confirmation--ie-assertthen). Confirmation usually consists of:
    * Asserting return values from the SU).
    * Asserting state changes in the SUT.
    * Verifying interactions with the SUT Test Doubles.

## Test Specify Behavior, Not Implementation
BDD tests specify [behavior](https://jhumelsine.github.io/2024/06/14/unit-test-attributes.html#behavior-based), not the SUT implementation. This can be challenging, since behavior derives from the implementation, and implementation is based upon behavior.

When I was first learning TDD, I realized I had created a test where the __Then__ portion only verified that a Spy Test Double had recorded that three methods in the SUT had been called. At first, I was pleased with the code coverage provided by the test, until I realized that the test verified no behavior. It only verified that the SUT method called three other methods in the class. If I were to refactor the code affecting these methods, then the test would fail even if the behavior had not changed. I reconsidered the behaviors in the SUT, and created a new test that asserted the SUT behaviors rather than verifying the implementation.

Since BDD tests are based upon behavior, we should be able to refactor the implementation without breaking the tests. Refactoring is changing the implementation without changing the behavior. Refactoring is used to make the code cleaner, more modular, more flexible, etc. without breaking existing behaviors.

Tests that verify implementation, such as my three method Spy verification described above, are brittle. Refactoring the code may break the tests even if no functional behavior has been changed. Some developers abandon automated testing, when test maintenance requires more effort than implementation maintenance. These developers have probably written brittle implementation specific tests even if unknowingly or unintentionally.

Test Doubles leak implementation details into tests. Tests with more Test Doubles will be more brittle than tests with fewer Test Doubles. More Test Doubles correlates with higher complexity in the implementation. If practicing TDD/BDD, then the number of Test Doubles shouldn't grow too large. Developers, who practice TDD/BDD and create complex tests and implementations, are taking extraordinary efforts to make their lives more miserable.

# When is Testing Enough, Enough?
__How do software developers know when they have created enough tests to reach sufficient confidence in their code?__

_When they can no longer create any failing tests._

This is where domain knowledge and creativity come into play.

## Domain Knowledge
At the start of my career in the 1980s, our system engineers would publish documents describing the system. These tended to be long sometimes meandering descriptions of the system.

The engineers would schedule a document review with the developers and testers. We’d review the document a paragraph at a time. If a sentence or paragraph sounded functional rather than just providing additional context, then we’d mark it with a yellow highlighter pen.

The documents were often vague, ambiguous or even missing important details. Heaven help us if we let anything slip through, because after document signoff, the engineers disappeared to their next project or release, and developers and testers were left holding the bag.

Things improved over the years with enumerated requirements and use cases, but a lot was still lost in translation when it came down to describing the behavior of the system.

Bob Martin tells a 3-4 minute story on the [Clean Coders Podcast](https://topenddevs.com/podcasts/clean-coders-podcast/episodes/cc-010-the-programmer-s-oath-with-robert-uncle-bob-martin-a7d7880e-4f66-417e-95d0-57b91319cb80#player1?catid=0&trackid=0) starting at around minute 36. It’s worth a listen, but here’s a quick summary. Bob’s team was paid when features were delivered. Feature completeness was defined by Acceptance Criteria/Tests provided by the customer. When the tests passed, the feature was delivered then Bob’s team was paid. If the customer was not satisfied with the feature, then the responsibility was theirs, since they defined the test. Bob’s story describes [Consumer-Driven Contract Testing]( https://microsoft.github.io/code-with-engineering-playbook/automated-testing/cdc-testing/) (CDC Testing), yet a topic for another blog (TBD).

Most behaviors should trace back to Domain Experts, whether external customers or internal product managers/owners, business analysts, etc. One source of behavior could be User Stories, which should contain Given-When-Then Acceptance Criteria, from which Given-When-Then Acceptance Tests can be derived.

If domain experts don’t follow through with Acceptance Criteria, then discuss user stories and feature behaviors with the domain experts. When a new or updated behavior arises in these conversations, consider how the hypothetical scenarios can be documented using Acceptance Criteria, from which derives the Acceptance Tests.

Functional Acceptance Tests should be domain specific enough that domain experts, product managers/owners, and other non-tech staff should be able to comprehend the intent. They may not understand all the details, but they should be able to confirm that it matches their intentions.

Some behaviors may be non-functional, such as performance. Some behaviors may be for individual classes and methods. These tend to be the responsibility of the architects and developers and probably not of great interest to the product Domain Experts.

## Equivalence Partitioning
We can’t write tests for every possible input value. We can organize all possible values into similar categories base using [Equivalence Partitioning](https://en.wikipedia.org/wiki/Equivalence_partitioning). Values in the same partition should exhibit the same behaviors. Rather than attempting to test every possible value in a partition, a few values or even one value in that partition should be sufficient for enough confidence for all values in the partition. Each partition category will probably have its own test.

As an example, if I were testing a square root function, I’d consider the following partitions:
* Square root of -1, which is undefined for real numbers, so I’d expect it to throw an exception.
* Square root of 0 returning 0.
* Square root of 2 returning 1.41421356237.
* Square root of 4 returning 2.
* Square root of 100000000000000 returning 10000000.

Each domain has its own domain specific partitions.

Consider traditional proofs in Computer Science. They prove that the data structure or algorithm is correct. They do not prove that the implementation is correct.

_Beware of bugs in the above code; I have only proved it correct, not tried it._ — __Donald Knuth__

While partitioning is not a proof, it mimics some aspects of proofs. Partitioning is like `cases` in proofs. But well design automated tests, which cover all the cases via partitioning, do somethings that Dijkstra's formal proofs can never do. Automated tests challenge the behavior as represented in the implementation upon each execution, which should happen frequently if practicing TDD and as a whole if using CI/CD pipelines.

In all fairness to Dijkstra, I suspect his quote about tests never showing the absence of bugs was during the age of manual testing, which tended to test the system as a whole. Current testing technology allows us to automate tests that challenge the code at almost any granular level in almost any scenario repeatedly.

## Degenerate, Edge and Boundary Tests
_The degenerate case is easy. I just return an empty list. For example, let’s say that I am writing a function to square a list of integers. E.g. [1 2 3 4] -> [1 4 9 16]. The Z test is simple. []->[]. The O test is easy to write: [2]->[4]._ — [Bob Martin](https://x.com/unclebobmartin/status/1313501871408123905)

The degenerate case is simple. Define tests for the most basic behaviors first. I did this with [Yuri](https://jhumelsine.github.io/2024/07/22/yuri-evaluator.html) with [the first test](https://jhumelsine.github.io/2024/07/22/yuri-evaluator.html#the-first-test), which evaluated “3” and as `3`. Then we added more complex test behavior scenarios while keeping all tests passing.

Include edge and boundary tests. Off-by-one errors are all too common. Define tests that confirm behavior at the edges and on all sides of a boundary.

# Test-Driven Development and Behavior-Driven Development
How does [Test-Driven Development](https://jhumelsine.github.io/2024/07/15/tdd.html) (TDD) and BDD relate to one another? Many consider TDD for unit testing and BDD for higher level integration or acceptance testing. Unit testing confirms nut-and-bolts; whereas integration testing confirms that the bolt actually screws into the nut. Lower level testing confirms software components and higher level testing confirms the integration of those software components as they form the product.

I don’t make discrete distinction. It's more continuous to me. I tell that the behavior concepts of BDD help us in defining unit tests as well. The major distinction I make between unit or integration/acceptance testing is the scope of the SUT. Unit tests tend to have smaller SUTs. Integration/Acceptance tests tend to have larger SUTs. Regardless of the SUT scope, we still want tests that confirm the behaviors bounded by the scope of the SUTs.

Traditional TDD and BDD frameworks are a bit different. TDD tends to use [xUnit]( https://en.wikipedia.org/wiki/XUnit ), with [JUnit](https://en.wikipedia.org/wiki/JUnit) being the Java version. BDD tends to use [Cucumber](https://en.wikipedia.org/wiki/Cucumber_(software)), [FitNesse](https://en.wikipedia.org/wiki/FitNesse) and others.

TDD and BDD complement one another like chocolate and peanut butter in Reese’s candy. While TDD and BDD do not depend upon one another, I feel each enhances the other:
* TDD is about how to write a test. BDD is about what to test and its structure and form.
* TDD is knowledge. BDD is wisdom.
* TDD is syntax. BDD is semantics.
* TDD is tactics. BDD is strategy.

# Summary
Test-Driven Development describes how to create a test. Behavior-Driven Development describes what test to create. 

Behavior-Driven Development helps us know when we’ve created enough tests to achieve confidence in our code.

Both practices complement one another.

# References
* Previous [blog references](https://jhumelsine.github.io/2024/06/07/unit-test-convert.html#references)
* https://en.wikipedia.org/wiki/Behavior-driven_development
* https://www.agilealliance.org/glossary/bdd/
* https://semaphoreci.com/community/tutorials/behavior-driven-development
* https://www.browserstack.com/guide/what-is-bdd
* https://www.spiceworks.com/tech/devops/articles/what-is-bdd/
* https://www.techtarget.com/searchsoftwarequality/definition/Behavior-driven-development-BDD
* https://www.functionize.com/automated-testing/behavior-driven-development
* https://inviqa.com/blog/bdd-guide
* https://smartbear.com/product/testleft/features/behavior-driven-development/
* https://www.behaviourdriven.org/
* https://www.altexsoft.com/blog/behavior-driven-development/
* https://www.youtube.com/watch?v=VS6EEUVZGLE
* https://dannorth.net/introducing-bdd/
* https://katalon.com/resources-center/blog/bdd-testing
* https://automationpanda.com/bdd/
* https://learningloop.io/glossary/behavior-driven-development
* https://www.inflectra.com/Ideas/Topic/Behavior-Driven-Development.aspx
* https://www.pmi.org/disciplined-agile/da-flex-toc/benefits-bdd
* https://www.launchnotes.com/glossary/behavior-driven-development-bdd-in-product-management-and-operations
* https://testrigor.com/blog/what-is-behavior-driven-development-bdd/
* https://www.youtube.com/watch?v=EZ05e7EMOLM

